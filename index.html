<html>

<head>
    <link rel='shortcut icon' href='./logo1.jpg'/>
    <meta charset="UTF-8">
    <title>Zhun Zhong's Homepage</title>
    <style type="text/css" media="screen">

        html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p,
        blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em,
        font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt,
        dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot,
        thead, tr, th, td {
            border: 0pt none;
            font-family: inherit;
            font-size: 100%;
            font-style: inherit;
            font-weight: inherit;
            margin: 0pt;
            outline-color: invert;
            outline-style: none;
            outline-width: 0pt;
            padding: 0pt;
            vertical-align: baseline;
        }

        a {
            color: #1772d0;
            text-decoration: none;
        }

        a:focus, a:hover {
            color: #f09228;
            text-decoration: none;
        }

        a.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        b.paper {
            font-weight: bold;
            font-size: 12pt;
        }

        * {
            margin: 0pt;
            padding: 0pt;
        }

        body {
            position: relative;
            margin: 3em auto 2em auto;
            width: 800px;
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
            background: #eee;
        }

        h2 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 15pt;
            font-weight: 700;
        }

        h4 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 13pt;
            font-weight: 700;
        }

        h3 {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 16px;
            font-weight: 700;
        }

        strong {
            font-family: Lato, Verdana, Helvetica, sans-serif;
            font-size: 14px;
        }

        ul {
            list-style: circle;
        }

        img {
            border: none;
        }

        li {
            padding-bottom: 0.5em;
            margin-left: 1.4em;
        }

        strong, b {
            font-weight: bold;
        }

        em, i {
            font-style: italic;
        }

        div.section {
            clear: both;
            margin-bottom: 1.5em;
            background: #eee;
        }

        div.spanner {
            clear: both;
        }

        div.paper {
            clear: both;
            margin-top: 0.3em;
            margin-bottom: 0.3em;
            border: 1px solid #ddd;
            background: #fff;
            padding: 1em 1em 1em 1em;
        }

        div.paper div {
            padding-right: 230px;
        }

        div.paper div.wide {
            padding-right: 0px;
        }

        img.paper {
            margin-bottom: 0.5em;
            float: right;
            width: 200px;
        }

        span.blurb {
            font-style: italic;
            display: block;
            margin-top: 0.75em;
            margin-bottom: 0.5em;
        }

        pre, code {
            font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
            margin: 1em 0;
            padding: 0;
        }

        div.paper pre {
            font-size: 0.9em;
        }

    </style>

    <script type="text/javascript">
        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-7953909-1']);
        _gaq.push(['_trackPageview']);

        (function () {
            var ga = document.createElement('script');
            ga.type = 'text/javascript';
            ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(ga, s);
        })();
    </script>

    <script type="text/javascript" src="js/hidebib.js"></script>

    <link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic"
          rel="stylesheet" type="text/css"/>
</head>

<body>

<div style="margin-bottom: 1em; border: 2px solid #eee; background-color: #fff; padding: 1em; height: 230px; border-radius: 10px;">
    <!-- Set button -->
    <div style="font-size:15pt; padding-left: 39.5em; vertical-align: top; height: 00px; width:100px;">
        <!--<p style="text-align:center">-->
        <!--<h1  style="text-align:center"><name>Amish Mittal</name></h1>-->
        <!--</p>-->
        <p style="text-align:left">
            <a href="#news" onclick=Expand("news")>News</a>
            <br/><br/>
            <a href="#publications" onclick=Expand("publications")>Publications</a>
            <br/><br/>
            <a href="#activities" onclick=Expand("activities")>Activities</a>
            <br/><br/>
            <a href="mailto:remove-this-if-you-are-human-zhunzhong007@gmail.com">Contact</a>
            <br/><br/>
        </p>
    </div>
    <!-- Set button -->
    <div style="margin: 0 auto; width: 70%; line-height: 130%;">
        <img title="Zhun Zhong" style="float: right; padding-right: .7em; height:
        200px;"
             src="./my_1.jpg"
             onmouseover="document.getElementById('mypic').src='./my_2.jpg';"
             onmouseout="document.getElementById('mypic').src='./my_1.jpg';"
             id="mypic"
        />
        <br/><br/>
        <div style="padding-right: .5em; vertical-align: top; height: 70px;">
            <span style="font-size: 20pt; line-height: 130%;"> Ph.D. Zhun Zhong 钟准</span><br/><br/>
            <span>Postdoc at University of Trento</span><br/><br/>
            <img border="0" src="./email.png"/><br/><br/>
            <a href="https://scholar.google.com/citations?user=nZizkQ0AAAAJ&hl=en">Google Scholar</a>/
            <a href="https://github.com/zhunzhong07">GitHub</a>
        </div>
    </div>
</div>

<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->
<div style="clear: both;">
</div>
<div class="section">
    <h2>About me</h2>
    <div class="paper">
        Hello! <br/><br/>
        I am currently a Postdoc in the Multimedia and Human Understanding Group (<a
            href="http://mhug.disi.unitn.it/">MHUG</a>) at the University of Trento, under the supervision of <a
            href="http://disi.unitn.it/~sebe/">Prof. Nicu Sebe</a>. I received my Ph.D. degree in 2019
        from the Cognitive Science Department at <strong>Xiamen University</strong> under the supervision of <a
            href="http://information.xmu.edu.cn/portal/node/106">Prof. Shaozi Li </a>. I was also a joint Ph.D.
        student in 2017-2019 at University of Technology Sydney under the co-supervision of <a
            href="https://sites.google.com/site/ianyyang2016/">Prof. Yi Yang</a> and <a
            href="http://www.liangzheng.com.cn">Dr. Liang Zheng.</a> I received the M.S. Degree in Computer Science
        and Technology in 2015 from <strong>China University Of Petroleum</strong>, Qingdao, China. I received the
        B.S. degree from the Information Engineering Department, from <strong>East China University of
        Technology</strong> in 2012. <br/><br/>
        My research interests include <strong>person re-identification</strong>, <strong>novel class discovery</strong>
        and <strong>domain
        adaptation</strong>.
    </div>
</div>


<div class="section">
    <h2 id="news"> News</h2>
    <div class="paper">
        <ul>
            <li><strong>[2022.8.10]</strong> One paper about attack and defense person re-identification is accepted to
                TPAMI 2022. Congrats to
                Fengxiang and Juanjuan. <img src="content/img/red-rss.png"/></li>
            <li><strong>[2022.7.15]</strong> Three papers about Domain Generalized Semantic Segmentation, Novel Class
                Disocvery and 3D Generation are accepted to ECCV 2022. Congrats to
                all. <img src="content/img/red-rss.png"/></li>
            <li><strong>[2022.6.2]</strong> One paper about Domain Generalized Semantic Segmentation is accepted to
                TCSVT 2022. Congrats to
                Yuyang. <img src="content/img/red-rss.png"/></li>
            <li><strong>[2022.5.10]</strong> One paper about cross-view geo-localization is accepted to TIP 2022.
                Congrats to Jinliang. <img src="content/img/red-rss.png"/></li>
            <li><strong>[2022.3.2]</strong> One paper about novel class discovery is accepted to CVPR 2022. Congrats to
                Yuyang. <img src="content/img/red-rss.png"/></li>
            <li><strong>[2022.2.16]</strong> I am invited to serve as an Area Chair (AC) and Open Souce Competition
                Chair for <a
                        href="https://2022.acmmm.org/">ACM Multimedia 2022</a>. <img src="content/img/red-rss.png"/>
            </li>
            <li><strong>[2021.10.31]</strong> Received Outstanding Reviewer Award at <a
                    href="https://nips.cc/Conferences/2021/ProgramCommittee">NeurIPS 2021</a>. <img
                    src="content/img/red-rss.png"/></li>
            <li><strong>[2021.7.23]</strong> One paper about novel class discovery is accepted to ICCV 2021 as Oral.
                Congrats to Enrico. <img src="content/img/rss-2x.png"/></li>
            <li><strong>[2021.3.3]</strong> Five papers are accepted to CVPR 2021. Congrats to Fengxiang, Yuyang and
                Subhankar. <img src="content/img/rss-2x.png"/></li>
            <li><strong>[2020.12.01]</strong> One paper is accepted to AAAI 2021. Congrats to Fengxiang. <img
                    src="content/img/rss-2x.png"/></li>
            <li><strong>[2020.6.20]</strong> Received Outstanding Reviewer Award at <a
                    href="http://cvpr2020.thecvf.com/reviewer-acknowledgements#outstanding-reviewers">CVPR 2020</a>.
                <img src="content/img/rss-2x.png"/></li>
            <li><strong>[2020.02.20]</strong> One paper is accepted to TPAMI 2020. <img
                    src="content/img/rss-2x.png"/></li>
            <li><strong>[2019.12.01]</strong> One paper is accepted to TMM 2020. <img src="content/img/rss-2x.png"/>
            </li>
            <li><strong>[2018.09.30]</strong> One paper is accepted to TIP 2019. <img src="content/img/rss-2x.png"/>
            </li>
            <li><strong>[2018.07.07]</strong> One paper is accepted to ECCV 2018. <img
                    src="content/img/rss-2x.png"/></li>
            <li><strong>[2018.02.24]</strong> One paper is accepted to CVPR 2018. <img
                    src="content/img/rss-2x.png"/></li>
        </ul>
    </div>
</div>


<div class="section">
    <h2 id="reports">Preprints</h2>

    <!--------------------------------------------------------------------------->
    <div class="paper" id="AdvStyle">
        <img class="paper"
             title="Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation"
             src="./img/AdvStyle.png"/>
        <div>
            <a class="paper" href="https://openreview.net/pdf?id=L_sHGieq1D" target="_blank">Adversarial Style
                Augmentation for Domain Generalized Urban-Scene Segmentation
            </a><br/>
            <strong>Zhun Zhong</strong>, Yuyang Zhao, Gim Hee Lee, Nicu Sebe<br/>
            Under review, 2022<br/>
            <a shape="rect" href="javascript:togglebib('AdvStyle')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://openreview.net/pdf?id=L_sHGieq1D" class="_blank">pdf</a> /
            <!--<a shape="rect" href="https://github.com/HeliosZhao/SFOCDA" class="_blank">code</a> /-->
            <pre xml:space="preserve">
@article{zhong2022advstyle,
  title={Adversarial Style Augmentation for Domain Generalized Urban-Scene Segmentation},
  author={Zhong, Zhun and Zhao, Yuyang and Lee, Gim Hee and Sebe, Nicu },
  journal={Under Review},
  year={2022}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="FedReID">
        <img class="paper"
             title="Federated and Generalized Person Re-identification through Domain and Feature Hallucinating"
             src="./img/FedReID.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2203.02689.pdf" target="_blank">Federated and Generalized
                Person Re-identification through Domain and Feature Hallucinating
            </a><br/>
            Fengxiang Yang, <strong>Zhun Zhong</strong>, Zhiming Luo, Shaozi Li, Nicu Sebe<br/>
            Arxiv, 2022<br/>
            <a shape="rect" href="javascript:togglebib('FedReID')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2203.02689.pdf" class="_blank">pdf</a> /
            <!--<a shape="rect" href="https://github.com/HeliosZhao/SFOCDA" class="_blank">code</a> /-->
            <pre xml:space="preserve">
@article{yang2022fedreid,
  title={Federated and Generalized Person Re-identification through Domain and Feature Hallucinating},
  author={Yang, Fengxiang and Zhong, Zhun and Luo, Zhiming and Li, Shaozi and Sebe, Nicu},
  journal={Arxiv},
  year={2022}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


</div>
<!--------------------------------------------------------------------------->

<div class="section">
    <h2 id="publications">Publications</h2>


    <!--------------------------------------------------------------------------->
    <div class="paper" id="metaAD">
        <img class="paper"
             title="Towards Robust Person Re-Identification by Defending Against Universal Attackers"
             src="./img/metapami.png"/>
        <div>
            <a class="paper" href="https://ieeexplore.ieee.org/document/9858024" target="_blank">Towards Robust Person
                Re-Identification by Defending Against Universal Attackers</a><br/>
            Fengxiang Yang, Juanjuan Weng, <strong>Zhun Zhong</strong>, Hong Liu, Zheng Wang, Zhiming Luo, Donglin Cao,
            Shaozi Li, Shin'ichi Satoh, Nicu Sebe<br/>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2022<br/>
            <a shape="rect" href="javascript:togglebib('metaAD')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://ieeexplore.ieee.org/document/9858024" class="_blank">pdf</a> /
            <a shape="rect"
               href="https://github.com/WJJLL/Meta-Attack-Defense"
               class="_blank">Code</a>
            <pre xml:space="preserve">
@ARTICLE{metaAD,
  author={Yang, Fengxiang and Weng, Juanjuan and Zhong, Zhun and Liu, Hong and Wang, Zheng and Luo, Zhiming and Cao, Donglin and Li, Shaozi and Satoh, Shin&#x0027;ichi and Sebe, Nicu},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  title={Towards Robust Person Re-Identification by Defending Against Universal Attackers},
  year={2022},
  volume={},
  number={},
  pages={1-17},
  doi={10.1109/TPAMI.2022.3199013}}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="INCD">
        <img class="paper"
             title="Class-incremental Novel Class Discovery" src="./img/INCD.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/2207.08605" target="_blank">Class-incremental Novel Class
                Discovery</a><br/>
            Subhankar Roy, Mingxuan Liu, <strong>Zhun Zhong</strong>, Nicu Sebe, Elisa Ricci<br/>
            European Conference on Computer Vision (ECCV), 2022<br/>
            <a shape="rect" href="javascript:togglebib('INCD')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/abs/2207.08605" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/OatmealLiu/class-iNCD" class="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{incd2022,
title={Class-incremental Novel Class Discovery},
author={Roy, Subhankar and Liu, Mingxuan and Zhong, Zhun and Sebe, Nicu and Ricci, Elisa},
booktitle={ECCV},
year={2022}}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="SHDC">
        <img class="paper"
             title="Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation"
             src="./img/SHDC.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2204.02548.pdf" target="_blank">Style-Hallucinated Dual
                Consistency Learning for Domain Generalized Semantic Segmentation
            </a><br/>
            Yuyang Zhao, <strong>Zhun Zhong</strong>, Na Zhao, Nicu Sebe, Gim Hee Lee<br/>
            European Conference on Computer Vision (ECCV), 2022<br/>
            <a shape="rect" href="javascript:togglebib('SHDC')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2204.02548.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/HeliosZhao/SHADE" class="_blank">code</a> /
            <!--<a shape="rect" href="https://github.com/HeliosZhao/SFOCDA" class="_blank">code</a> /-->
            <pre xml:space="preserve">
            @inproceedings{zhao2022shdc,
            title={Style-Hallucinated Dual Consistency Learning for Domain Generalized Semantic Segmentation},
            author={Zhao, Yuyang and Zhong, Zhun and Zhao, Na and Sebe, Nicu and Lee, Gim Hee},
            booktitle={ECCV},
            year={2022}
                }
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="3dgan">
        <img class="paper"
             title="3D-Aware Semantic-Guided Generative Model for Human Synthesis" src="./img/3dgan.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2112.01422.pdf" target="_blank">3D-Aware Semantic-Guided
                Generative Model for Human Synthesis</a><br/>
            Jichao Zhang, Enver Sangineto, Hao Tang, Aliaksandr Siarohin, <strong>Zhun Zhong</strong>, Nicu Sebe, Wei
            Wang<br/>
            European Conference on Computer Vision (ECCV), 2022<br/>
            <a shape="rect" href="javascript:togglebib('3dgan')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2112.01422.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/zhangqianhui/3DSGAN" class="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{zhang20213d,
  title={3D-Aware Semantic-Guided Generative Model for Human Synthesis},
  author={Zhang, Jichao and Sangineto, Enver and Tang, Hao and Siarohin, Aliaksandr and Zhong, Zhun and Sebe, Nicu and Wang, Wei},
  booktitle={ECCV},
  year={2022}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="SFOCDA">
        <img class="paper"
             title="Source-Free Open Compound Domain Adaptation in Semantic Segmentation"
             src="./img/SFOCDA.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2106.03422" target="_blank">Source-Free Open Compound Domain
                Adaptation in Semantic Segmentation
            </a><br/>
            Yuyang Zhao, <strong>Zhun Zhong</strong>, Zhiming Luo, Gim Hee Lee, Nicu Sebe<br/>
            IEEE Transactions on Circuits and Systems for Video Technology (TCSVT), 2022<br/>
            <a shape="rect" href="javascript:togglebib('SFOCDA')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2106.03422" class="_blank">pdf</a> /
            <!--<a shape="rect" href="https://github.com/HeliosZhao/SFOCDA" class="_blank">code</a> /-->
            <pre xml:space="preserve">
@article{zhao2021source,
  title={Source-Free Open Compound Domain Adaptation in Semantic Segmentation},
  author={Zhao, Yuyang and Zhong, Zhun and Luo, Zhiming and Lee, Gim Hee and Sebe, Nicu},
  journal={IEEE Transactions on Circuits and Systems for Video Technology},
  year={2022}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="rknet">
        <img class="paper"
             title="Joint Representation Learning and Keypoint Detection for Cross-view Geo-localization"
             src="./img/rknet.png"/>
        <div>
            <a class="paper" href="paper/RK_Net.pdf" target="_blank">Joint Representation Learning and Keypoint
                Detection for Cross-view Geo-localization</a><br/>
            Jinliang Lin, Zhedong Zheng, <strong>Zhun Zhong</strong>, Zhiming Luo, Shaozi Li, Yi Yang, Nicu Sebe<br/>
            IEEE Transactions on Image Processing (TIP), 2022<br/>
            <a shape="rect" href="javascript:togglebib('rknet')" class="togglebib">bibtex</a> /
            <a shape="rect" href="paper/RK_Net.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/AggMan96/RK-Net" class="_blank">Code</a>
            <pre xml:space="preserve">
@article{lin2022,
                title={Joint Representation Learning and Keypoint Detection for Cross-view Geo-localization},
                author={Lin, Jinliang and Zheng, Zhedong and Zhong, Zhun and Luo, Zhiming and Li, Shaozi and Yang, Yi and Sebe, Nicu},
                journal={IEEE Transactions on Image Processing (TIP)},
                year={2022},
                }
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="ncdss">
        <img class="paper"
             title="Novel Class Discovery in Semantic Segmentation" src="./img/ncdss.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2112.01900.pdf" target="_blank">Novel Class Discovery in
                Semantic Segmentation</a><br/>
            Yuyang Zhao, <strong>Zhun Zhong</strong>, Nicu Sebe, Gim Hee Lee<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022<br/>
            <a shape="rect" href="javascript:togglebib('ncdss')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2112.01900.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://ncdss.github.io/" class="_blank">Project Page</a> /
            <a shape="rect" href="https://github.com/HeliosZhao/NCDSS" class="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{zhao2022ncdss,
title={Novel Class Discovery in Semantic Segmentation},
author={Zhao, Yuyang and Zhong, Zhun and Sebe, Nicu and Lee, Gim Hee},
booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
year={2022}}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="auo">
        <img class="paper"
             title="A Unified Objective for Novel Class Discovery" src="./img/iccv21.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/2108.08536" target="_blank">A Unified Objective for Novel Class
                Discovery</a><br/>
            Enrico Fini, Enver Sangineto, Stéphane Lathuilière, <strong>Zhun Zhong</strong>, Moin Nabi, Elisa Ricci<br/>
            IEEE International Conference on Computer Vision (ICCV), Oral, 2021<br/>
            <a shape="rect" href="javascript:togglebib('auo')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/abs/2108.08536" class="_blank">pdf</a> /
            <a shape="rect" href="https://ncd-uno.github.io/" class="_blank">Project Page</a> /
            <a shape="rect" href="https://github.com/DonkeyShot21/UNO" class="_blank">Code</a>
            <pre xml:space="preserve">
@InProceedings{Fini_2021_ICCV,
    author    = {Fini, Enrico and Sangineto, Enver and Lathuilière, Stéphane and Zhong, Zhun and Nabi, Moin and Ricci, Elisa},
    title     = {A Unified Objective for Novel Class Discovery},
    booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
    year      = {2021}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="ncl">
        <img class="paper"
             title="Neighborhood Contrastive Learning for Novel Class Discovery" src="./project/img/ncl_framework.png"/>
        <div>
            <a class="paper" href="***" target="_blank">Neighborhood Contrastive Learning for Novel Class
                Discovery</a><br/>
            <strong>Zhun Zhong</strong>, Enrico Fini, Subhankar Roy, Zhiming Luo, Elisa Ricci, Nicu Sebe<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br/>
            <a shape="rect" href="javascript:togglebib('ncl')" class="togglebib">bibtex</a> /
            <a shape="rect"
               href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_Neighborhood_Contrastive_Learning_for_Novel_Class_Discovery_CVPR_2021_paper.pdf"
               class="_blank">pdf</a> /
            <a shape="rect" href="http://zhunzhong.site/project/ncl.html" class="_blank">Project Page</a> /
            <pre xml:space="preserve">
@InProceedings{Zhong_2021_CVPR,
    author    = {Zhong, Zhun and Fini, Enrico and Roy, Subhankar and Luo, Zhiming and Ricci, Elisa and Sebe, Nicu},
    title     = {Neighborhood Contrastive Learning for Novel Class Discovery},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {10867-10875}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="openmix">
        <img class="paper"
             title="OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World"
             src="./project/img/openmix_abs.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/2004.05551" target="_blank">OpenMix: Reviving Known Knowledge
                for Discovering Novel Visual Categories in An Open World
            </a><br/>
            <strong>Zhun Zhong</strong>, Linchao Zhu, Zhiming Luo, Shaozi Li, Yi Yang, Nicu Sebe<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br/>
            <a shape="rect" href="javascript:togglebib('openmix')" class="togglebib">bibtex</a> /
            <a shape="rect"
               href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhong_OpenMix_Reviving_Known_Knowledge_for_Discovering_Novel_Visual_Categories_in_CVPR_2021_paper.pdf"
               class="_blank">pdf</a> /
            <a shape="rect" href="http://zhunzhong.site/project/openmix.html" class="_blank">Project Page</a> /
            <pre xml:space="preserve">
@inproceedings{zhong2021openmix,
  title={OpenMix: Reviving Known Knowledge for Discovering Novel Visual Categories in An Open World},
  author={Zhong, Zhun and Zhu, Linchao and Luo, Zhiming and Li, Shaozi and Yang, Yi and Sebe, Nicu},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="MetaCam">
        <img class="paper"
             title="Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification"
             src="./img/metacam.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/2103.04618.pdf" target="_blank">Joint Noise-Tolerant Learning
                and
                Meta Camera Shift Adaptation for Unsupervised Person Re-Identification</a><br/>
            Fengxiang Yang, <strong>Zhun Zhong</strong>, Zhiming Luo, Yuanzheng Cai, Shaozi Li, Nicu Sebe<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br/>
            <a shape="rect" href="javascript:togglebib('MetaCam')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/2103.04618.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/FlyingRoastDuck/MetaCam_DSCE" class="_blank">code</a>
            <pre xml:space="preserve">
@inproceedings{Yang2021MetaCam,
  title={Joint Noise-Tolerant Learning and Meta Camera Shift Adaptation for Unsupervised Person Re-Identification},
  author={Yang, Fengxiang and Zhong, Zhun and Luo, Zhiming and Yuanzheng, Cai and Li, Shaozi and Sebe, Nicu},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}
            </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="M3L">
        <img class="paper"
             title="Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification"
             src="./img/m3l_framework.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/2012.00417" target="_blank">Learning to Generalize Unseen
                Domains
                via Memory-based Multi-Source Meta-Learning for Person Re-Identification
            </a><br/>
            Yuyang Zhao, <strong>Zhun Zhong</strong>, Fengxiang Yang, Zhiming Luo, Shaozi Li, Nicu Sebe<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br/>
            <a shape="rect" href="javascript:togglebib('M3L')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/abs/2012.00417" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/HeliosZhao/M3L" class="_blank">code</a> /
            <pre xml:space="preserve">
@inproceedings{Zhao2021M3L,
  title={Learning to Generalize Unseen Domains via Memory-based Multi-Source Meta-Learning for Person Re-Identification},
  author={Zhao, Yuyang and Zhong, Zhun and Yang, Fengxiang and Luo, Zhiming and Li, Shaozi and Sebe, Nicu},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="CGCT">
        <img class="paper" title="Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation"
             src="./img/cgct_framework.png"/>
        <div>
            <a class="paper" href="***" target="_blank">Curriculum Graph Co-Teaching for Multi-Target Domain
                Adaptation</a><br/>
            Subhankar Roy, Evgeny Krivosheev, <strong>Zhun Zhong</strong>, Elisa Ricci, Nicu Sebe<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021<br/>
            <a shape="rect" href="javascript:togglebib('CGCT')" class="togglebib">bibtex</a> /
            <a shape="rect" href="***" class="_blank">pdf</a> /
            <a shape="rect" href="https://roysubhankar.github.io/graph-coteaching-adaptation/" class="_blank">Project
                Page</a> /
            <a shape="rect" href="https://github.com/Evgeneus/Graph-Domain-Adaptaion" class="_blank">Code</a> /
            <pre xml:space="preserve">
@inproceedings{Roy2021CGCT,
  title={Curriculum Graph Co-Teaching for Multi-Target Domain Adaptation},
  author={Roy, Subhankar and Krivosheev, Evgeny and Zhong, Zhun and Ricci, Elisa and Sebe, Nicu},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2021}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>


    <!--------------------------------------------------------------------------->
    <div class="paper" id="MetaAttack">
        <img class="paper"
             title="Learning to Attack Real-World Models for Person Re-identification via Virtual-Guided MetaLearning"
             src="./img/metaattack_framework.png"/>
        <div>
            <a class="paper" href="https://www.aaai.org/AAAI21Papers/AAAI-578.YangF.pdf" target="_blank">Learning to
                Attack Real-World Models for Person Re-identification via Virtual-Guided MetaLearning</a><br/>
            Fengxiang Yang, <strong>Zhun Zhong</strong>, Hong Liu, Zheng Wang, Zhiming Luo, Shaozi Li, Nicu Sebe,
            Shin’ichi Satoh<br/>
            AAAI, 2021<br/>
            <a shape="rect" href="javascript:togglebib('MetaAttack')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://www.aaai.org/AAAI21Papers/AAAI-578.YangF.pdf" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/FlyingRoastDuck/MetaAttack_AAAI21" class="_blank">code</a> /
            <pre xml:space="preserve">
@inproceedings{Yang2021MetaAttack,
  title={Learning to Attack Real-World Models for Person Re-identiﬁcation via Virtual-Guided MetaLearning},
  author={Yang, Fengxiang and Zhong, Zhun and Liu, Hong and Wang, Zheng and Luo, Zhiming and Li, Shaozi and Sebe, Nicu and Satoh, Shin’ichi},
  booktitle={AAAI},
  year={2021}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->

    <!--------------------------------------------------------------------------->
    <div class="paper" id="memory">
        <img class="paper" title="Learning to Adapt Invariance in Memory for Person Re-identification"
             src="./img/ecn_plus_framework.png"/>
        <div>
            <a class="paper" href="https://ieeexplore.ieee.org/abstract/document/9018132" target="_blank">Learning to
                Adapt
                Invariance in Memory for Person Re-identification
            </a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Zhiming Luo, Shaozi Li, Yi Yang<br/>
            IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020<br/>
            <a shape="rect" href="javascript:togglebib('memory')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://ieeexplore.ieee.org/abstract/document/9018132" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/zhunzhong07/ECN" target="_blank">Code</a>
            <pre xml:space="preserve">
@article{zhong2020memory,
                title={Learning to Adapt Invariance in Memory for Person Re-identification},
                author={Zhong, Zhun and Zheng, Liang and Luo, Zhiming and Li, Shaozi and Yang, Yi},
                journal={IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)},
                year={2021},
                volume={43},
                number={8},
                pages={2723-2738},
                doi={10.1109/TPAMI.2020.2976933},
                ISSN={1939-3539},
                month={Aug}
                }
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="Erasing">
        <img class="paper" title="Random Erasing Data Augmentation" src="./erasing.jpg"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/1708.04896" target="_blank">Random Erasing Data
                Augmentation</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Guoliang Kang, Shaozi Li, Yi Yang<br/>
            AAAI, Oral, 2020<br/>
            <a shape="rect" href="javascript:toggleabs('Erasing')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('Erasing')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/abs/1708.04896" target="_blank">PDF</a> /
            <a shape="rect"
               href="https://pytorch.org/docs/master/torchvision/transforms.html#torchvision.transforms.RandomErasing"
               target="_blank">Torchvision</a> /
            <a shape="rect" href="https://github.com/zhunzhong07/Random-Erasing" target="_blank">CIFAR</a> /
            <a shape="rect" href="https://github.com/rwightman/pytorch-image-models" target="_blank">ImageNet</a> /
            <a shape="rect" href="https://github.com/zhunzhong07/CamStyle" target="_blank">Re-ID</a> (1000+ Citations)
            <br/>
            <a shape="rect" href="https://www.paperdigest.org/2021/05/most-influential-aaai-papers-2021-05/"
               target="_blank"><font color="red">PaperDigest Most Influential AAAI Papers (1st-Place)</font></a>
            <pre xml:space="preserve">
@inproceedings{zhong2020erasing,
  title={Random Erasing Data Augmentation},
  author={Zhong, Zhun and Zheng, Liang and Kang, Guoliang and Li, Shaozi and Yang, Yi},
  booktitle={AAAI},
  year={2020}
}
        </pre>
            <span class="blurb">
In this paper, we introduce Random Erasing, a new data augmentation method for training the convolutional neural network (CNN). In training, Random Erasing randomly selects a rectangle region in an image and erases its pixels with random values. In this process, training images with various levels of occlusion are generated, which reduces the risk of over-fitting and makes the model robust to occlusion. Random Erasing is parameter learning free, easy to implement, and can be integrated with most of the CNN-based recognition models. Albeit simple, Random Erasing is complementary to commonly used data augmentation techniques such as random cropping and flipping, and yields consistent improvement over strong baselines in image classification, object detection and person re-identification.
        </span>
        </div>
        <div class="spanner"></div>
    </div>

    <!--------------------------------------------------------------------------->
    <div class="paper" id="cvpr19">
        <img class="paper" title="Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identification"
             src="./img/ecn_framework.png"/>
        <div>
            <a class="paper" href="https://arxiv.org/abs/1904.01990" target="_blank">Invariance Matters: Exemplar Memory
                for
                Domain Adaptive Person Re-identification</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Zhiming Luo, Shaozi Li, Yi Yang<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019<br/>
            <a shape="rect" href="javascript:togglebib('cvpr19')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/abs/1904.01990" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/zhunzhong07/ECN" target="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{zhong2019invariance,
  title={Invariance Matters: Exemplar Memory for Domain Adaptive Person Re-identiﬁcation},
  author={Zhong, Zhun and Zheng, Liang and Luo, Zhiming and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2019}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


    <!--------------------------------------------------------------------------->
    <div class="paper" id="camstyle-TIP">
        <img class="paper" title="CamStyle: A Novel Data Augmentation Method for Person Re-identification"
             src="./img/camstyle_abs.png"/>
        <div>
            <a class="paper" href="https://ieeexplore.ieee.org/document/8485427" target="_blank">CamStyle: A Novel Data
                Augmentation Method for Person Re-identification</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang<br/>
            IEEE Transactions on Image Processing (TIP), 2019<br/>
            <a shape="rect" href="javascript:togglebib('camstyle-TIP')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://ieeexplore.ieee.org/document/8485427" class="_blank">pdf</a> /
            <a shape="rect" href="https://github.com/zhunzhong07/CamStyle" target="_blank">Code</a>
            <pre xml:space="preserve">
@article{zhong2019camstyle,
  title={CamStyle: A Novel Data Augmentation Method for Person Re-identification},
  author={Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
  journal={IEEE Transactions on Image Processing},
  volume={28},
  number={3},
  pages={1176--1190},
  publisher={IEEE},
  year={2019}
}
        </pre>
        </div>
        <div class="spanner"></div>
    </div>
    <!--------------------------------------------------------------------------->


    <div class="paper" id="hhl">
        <img class="paper" title="Generalizing A Person Retrieval Model Hetero- and Homogeneously"
             src="./hhl_framework.png"/>
        <div>
            <a class="paper"
               href="http://openaccess.thecvf.com/content_ECCV_2018/html/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.html"
               target="_blank">Generalizing A Person Retrieval Model Hetero- and Homogeneously</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Shaozi Li, Yi Yang<br/>
            European Conference on Computer Vision (ECCV), 2018<br/>
            <a shape="rect" href="javascript:toggleabs('hhl')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('hhl')" class="togglebib">bibtex</a> /
            <a shape="rect"
               href="http://openaccess.thecvf.com/content_ECCV_2018/html/Zhun_Zhong_Generalizing_A_Person_ECCV_2018_paper.html"
               target="_blank">PDF</a>/
            <a shape="rect" href="https://github.com/zhunzhong07/HHL" target="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{zhong2018generalizing,
  title={Generalizing A Person Retrieval Model Hetero-and Homogeneously},
  author={Zhong, Zhun and Zheng, Liang and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={172--188},
  year={2018}
}
        </pre>
            <span class="blurb">
Person re-identiﬁcation (re-ID) poses unique challenges for unsupervised domain adaptation (UDA) in that classes in the source and target sets (domains) are entirely diﬀerent and that image variations are largely caused by cameras. Given a labeled source training set and an unlabeled target training set, we aim to improve the generalization ability of re-ID models on the target testing set. To this end, we introduce a Hetero-Homogeneous Learning (HHL) method. Our method enforces two properties simultaneously: 1) camera invariance, learned via positive pairs formed by unlabeled target images and their camera style transferred counterparts; 2) domain connectedness, by regarding source / target images as negative matching pairs to the target / source images. The ﬁrst property is implemented by homogeneous learning because training pairs are collected from the same domain. The second property is achieved by heterogeneous learning because we sample training pairs from both the source and target domains. On Market-1501, DukeMTMC-reID and CUHK03, we show that the two properties contribute indispensably and that very competitive re-ID UDA accuracy is achieved.
        </span>
        </div>
        <div class="spanner"></div>
    </div>


    <div class="paper" id="camstyle">
        <img class="paper" title="Camera Style Adaptation for Person Re-identification" src="./camstyle.jpg"/>
        <div>
            <a class="paper"
               href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.pdf"
               target="_blank">Camera Style Adaptation for Person Re-identification</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Zhedong Zheng, Shaozi Li, Yi Yang<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018<br/>
            <a shape="rect" href="javascript:toggleabs('camstyle')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('camstyle')" class="togglebib">bibtex</a> /
            <a shape="rect"
               href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Zhong_Camera_Style_Adaptation_CVPR_2018_paper.pdf"
               target="_blank">PDF</a>/
            <a shape="rect" href="https://github.com/zhunzhong07/CamStyle" target="_blank">Code</a>
            <pre xml:space="preserve">
@inproceedings{zhong2018camera,
  title={Camera style adaptation for person re-identification},
  author={Zhong, Zhun and Zheng, Liang and Zheng, Zhedong and Li, Shaozi and Yang, Yi},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={5157--5166},
  year={2018}
}
        </pre>
            <span class="blurb">
Being a cross-camera retrieval task, person re-identification suffers from image style variations caused by different cameras. The art implicitly addresses this problem by learning a camera-invariant descriptor subspace. In this paper, we explicitly consider this challenge by introducing camera style (CamStyle) adaptation. CamStyle can serve as a data augmentation approach that smooths the camera style disparities. Specifically, with CycleGAN, labeled training images can be style-transferred to each camera, and, along with the original training samples, form the augmented training set. This method, while increasing data diversity against over-fitting, also incurs a considerable level of noise. In the effort to alleviate the impact of noise, the label smooth regularization (LSR) is adopted. The vanilla version of our method (without LSR) performs reasonably well on few-camera systems in which over-fitting often occurs. With LSR, we demonstrate consistent improvement in all systems regardless of the extent of over-fitting. We also report competitive accuracy compared with the state of the art.        </span>
        </div>
        <div class="spanner"></div>
    </div>


    <div class="paper" id="Reranking-reid">
        <img class="paper" title="Re-ranking Person Re-identification with k-reciprocal Encoding"
             src="./person-re-ranking.jpg"/>
        <div>
            <a class="paper"
               href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf"
               target="_blank">Re-ranking Person Re-identification with k-reciprocal Encoding</a><br/>
            <strong>Zhun Zhong</strong>, Liang Zheng, Donglin Cao,Shaozi Li<br/>
            IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017<br/>
            <a shape="rect" href="javascript:toggleabs('Reranking-reid')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('Reranking-reid')" class="togglebib">bibtex</a> /
            <a shape="rect"
               href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Zhong_Re-Ranking_Person_Re-Identification_CVPR_2017_paper.pdf"
               target="_blank">PDF</a>/
            <a shape="rect" href="https://github.com/zhunzhong07/person-re-ranking" target="_blank">Code and CUHK03 new
                training/testing protocol</a>
            <pre xml:space="preserve">
@inproceedings{zhong2017re,
  title={Re-ranking person re-identification with k-reciprocal encoding},
  author={Zhong, Zhun and Zheng, Liang and Cao, Donglin and Li, Shaozi},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},
  pages={3652--3661},
  year={2017},
  organization={IEEE}
}
        </pre>
            <span class="blurb">
When considering person re-identification (re-ID) as a retrieval process, re-ranking is a critical step to improve its accuracy. Yet in the re-ID community, limited effort has been devoted to re-ranking, especially those fully automatic, unsupervised solutions. In this paper, we propose a k-reciprocal encoding method to re-rank the re-ID results. Our hypothesis is that if a gallery image is similar to the probe in the k-reciprocal nearest neighbors, it is more likely to be a true match. Specifically, given an image, a k-reciprocal feature is calculated by encoding its k-reciprocal nearest neighbors into a single vector, which is used for re-ranking under the Jaccard distance. The final distance is computed as the combination of the original distance and the Jaccard distance. Our re-ranking method does not require any human interaction or any labeled data, so it is applicable to large-scale datasets. Experiments on the large-scale Market-1501, CUHK03, MARS, and PRW datasets confirm the effectiveness of our method.

        </span>
        </div>
        <div class="spanner"></div>
    </div>


    <div class="paper" id="Re-proposals">
        <img class="paper" title="Class-Specific Object Proposals Re-ranking for Object Detection in Automatic Driving"
             src="./re-proposals.jpg"/>
        <div>
            <a class="paper" href="http://www.sciencedirect.com/science/article/pii/S0925231217303922" target="_blank">Class-Specific
                Object Proposals Re-ranking for Object Detection in Automatic Driving</a><br/>
            <strong>Zhun Zhong</strong>, Mingyi Lei, Donglin Cao, Jianping Fan, Shaozi Li<br/>
            Neurocomputing, 2017<br/>
            <a shape="rect" href="javascript:toggleabs('Re-proposals')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('Re-proposals')" class="togglebib">bibtex</a> /
            <a shape="rect" href="http://www.sciencedirect.com/science/article/pii/S0925231217303922"
               target="_blank">PDF</a>
            <pre xml:space="preserve">
@article{zhong2017class,
  title={Class-specific object proposals re-ranking for object detection in automatic driving},
  author={Zhong, Zhun and Lei, Mingyi and Cao, Donglin and Fan, Jianping and Li, Shaozi},
  journal={Neurocomputing},
  volume={242},
  pages={187--194},
  year={2017},
  publisher={Elsevier}
}
        </pre>
            <span class="blurb">
        Object detection often suffers from a plenty of bootless proposals, selecting high quality proposals
remains a great challenge. In this paper, we propose a semantic, class-specific approach to re-rank
object proposals, which can consistently improve the recall performance even with less proposals.
We first extract features for each proposal including semantic segmentation, stereo information,
contextual information, CNN-based objectness and low-level cue, and then score them using classspecific
weights learnt by Structured SVM. The advantages of the proposed model are two-fold: 1)
it can be easily merged to existing generators with few computational costs, and 2) it can achieve
high recall rate uner strict critical even using less proposals. Experimental evaluation on the KITTI
benchmark demonstrates that our approach significantly improves existing popular generators on
recall performance. Moreover, in the experiment conducted for object detection, even with 1,500
proposals, our approach can still have higher average precision (AP) than baselines with 5,000
proposals.

        </span>
        </div>
        <div class="spanner"></div>
    </div>


    <div class="paper" id="GCP">
        <img class="paper" title="Detecting Ground Control Points via Convolutional Neural Network for Stereo Matching"
             src="./mtap.jpg"/>
        <div>
            <a class="paper" href="http://rd.springer.com/article/10.1007/s11042-016-3932-y" target="_blank">Detecting
                Ground Control Points via Convolutional Neural Network for Stereo Matching</a><br/>
            <strong>Zhun Zhong</strong>, Songzhi Su, Donglin Cao, Shaozi Li, Zhihan Lv<br/>
            Multimedia Tools and Applications (<strong>MTA</strong>), 2016<br/>
            <a shape="rect" href="javascript:toggleabs('GCP')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('GCP')" class="togglebib">bibtex</a> /
            <a shape="rect" href="http://rd.springer.com/article/10.1007/s11042-016-3932-y" target="_blank">PDF</a>
            <pre xml:space="preserve">
@article{zhong2017detecting,
  title={Detecting ground control points via convolutional neural network for stereo matching},
  author={Zhong, Zhun and Su, Songzhi and Cao, Donglin and Li, Shaozi and Lv, Zhihan},
  journal={Multimedia Tools and Applications},
  volume={76},
  number={18},
  pages={18473--18488},
  year={2017},
  publisher={Springer}
}
        </pre>
            <span class="blurb">
        In this paper, we present a novel approach to detect ground control points (GCPs) for stereo matching problem. First of all, we train a convolutional neural network (CNN) on a large stereo set, and compute the matching confidence of each pixel by using the trained CNN model. Secondly, we present a ground control points selection scheme according to the maximum matching confidence of each pixel. Finally, the selected GCPs are used to refine the matching costs, then we apply the new matching costs to perform optimization with semi-global matching algorithm for improving the final disparity maps. We evaluate our approach on the KITTI 2012 stereo benchmark dataset. Our experiments show that the proposed approach significantly improves the accuracy of disparity maps.
        </span>
        </div>
        <div class="spanner"></div>
    </div>


    <div class="paper" id="icip">
        <img class="paper" title="Unsupervised domain adaption dictionary learning for visual recognition"
             src="./icip2015.jpg"/>
        <div>
            <a class="paper" href="https://arxiv.org/pdf/1506.01125.pdf" target="_blank">Unsupervised domain adaption
                dictionary learning for visual recognition</a><br/>
            <strong>Zhun Zhong</strong>, Zongming Li, Runlin Li, Xiaoxia Sun<br/>
            <strong>ICIP</strong>, 2015<br/>
            <a shape="rect" href="javascript:toggleabs('icip')" class="toggleabs">abstract</a> /
            <a shape="rect" href="javascript:togglebib('icip')" class="togglebib">bibtex</a> /
            <a shape="rect" href="https://arxiv.org/pdf/1506.01125.pdf" target="_blank">axXiv</a>
            <pre xml:space="preserve">
@article{zhong2015unsupervised,
  title={Unsupervised domain adaption dictionary learning for visual recognition},
  author={Zhong, Zhun and Li, Zongmin and Li, Runlin and Sun, Xiaoxia},
  journal={arXiv preprint arXiv:1506.01125},
  year={2015}
}
        </pre>
            <span class="blurb">
Over the last years, dictionary learning method has been extensively applied to deal with various computer vision recognition applications, and produced state-of-the-art results. However, when the data instances of a target domain have a different distribution than that of a source domain, the dictionary learning method may fail to perform well. In this paper, we address the cross-domain visual recognition problem and propose a simple but effective unsupervised domain adaption approach, where labeled data are only from source domain. In order to bring the original data in source and target domain into the same distribution, the proposed method forcing nearest coupled data between source and target domain to have identical sparse representations while jointly learning dictionaries for each domain, where the learned dictionaries can reconstruct original data in source and target domain respectively. So that sparse representations of original data can be used to perform visual recognition tasks. We demonstrate the effectiveness of our approach on standard datasets. Our method performs on par or better than competitive state-of-the-art methods.
        </span>
        </div>
        <div class="spanner"></div>
    </div>

</div>


<div class="section">
    <h2 id="activities">Activities</h2>
    <div class="paper">
        <ul>
            <li> Area Chair / Senior PC: IJCAI 2021, AAAI 2022, MM 2022</li>
            <li> I have given a talk at ECCV 2018 Tutorial: "Representation Learning in Pedestrian Re-identification".
            <li> Program Committee / Reviewer: IEEE TPAMI, IEEE TIP, TMLR, TCSVT, IEEE TMM, CVPR 2019-2022, ICCV
                2019-2021, ECCV 2020-2022,
                NeurIPS 2020-2021, AAAI 2019-2022, ACM MM 2020-2022, ICML 2022, ICLR 2022
            </li>
        </ul>
    </div>
</div>


<div class="section">
    <h2>My Friends & Collaborators</h2>
    <div class="paper">
        <ul>
            <strong><a href="https://sites.google.com/view/zhimingluo/">Zhiming Luo</a> &nbsp;&nbsp;XMU</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <strong><a href="https://flyingroastduck.github.io/">Fengxiang Yang</a> &nbsp;&nbsp;XMU</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <strong><a href="https://github.com/HeliosZhao/HeliosZhao.github.io">Yuyang Zhao</a>
                &nbsp;&nbsp;NUS</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <strong><a href="https://lancerlian.github.io/">Sheng Lian</a>
                &nbsp;&nbsp;XMU</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <br>
            <br>
            <strong><a href="https://lynnhongliu.github.io/hliu/">Hong Liu</a> &nbsp;&nbsp;NII</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <br>
            <br>
            <strong><a href="http://ffmpbgrnn.github.io/">Linchao Zhu</a> &nbsp;&nbsp;UTS</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <br>
            <br>
            <strong><a href="https://scholar.google.com/citations?user=YfzgrDYAAAAJ&hl=en">Subhankar Roy</a> &nbsp;&nbsp;UNITN</strong>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
            <br>
        </ul>
    </div>
</div>


<div style="clear:both;">
    <p align="right"><font size="2">
        <a href="https://jonbarron.info/" target="_blank">I like this website!</a>
    </font>
    </p>

    <br/>
</div>

<script xml:space="preserve" language="JavaScript">
    hideallbibs();
    hideallabs();
    </script>


<a href="http://www.easycounter.com/"><img alt="HTML Counter"
                                           src="http://www.easycounter.com/counter.php?xiaozhun07"
                                           border="0"></a> <i><font size="2" face="Arial">unique visitors
    since Dec 2016</font></i>

</body>
</html>
