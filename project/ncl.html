<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Neighborhood Contrastive Learning for Novel Class Discovery</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="In this paper, we address Novel Class Discovery (NCD), the task of unveiling new classes in a set of unlabeled samples given a labeled dataset with known classes. We exploit the peculiarities of NCD to build a new framework, named Neighborhood Contrastive Learning (NCL), to learn discriminative representations that are important to clustering performance. Our contribution is twofold. First, we ﬁnd that a feature extractor trained on the labeled set generates representations in which a generic query sample and its neighbors are likely to share the same class. We exploit this observation to retrieve and aggregate pseudo-positive pairs with contrastive learning, thus encouraging the model to learn more discriminative representations. Second, we notice that most of the instances are easily discriminated by the network, contributing less to the contrastive loss. To overcome this issue, we propose to generate hard negatives by mixing labeled and unlabeled samples in the feature space. We experimentally demonstrate that these two ingredients signiﬁcantly contribute to clustering performance and lead our model to outperform state-of-the-art methods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8% on ImageNet).".
>
<meta name="keywords" content="Novel Class Discovery; Neighborhood Contrastive Learning;">
<link rel="author" href="zhunzhong.site">

<!-- Fonts and stuff -->
<link href="./css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
<script async="" src="./prettify.js"></script>

</head>

<body>
  <div id="content">
    <div id="content-inner">

    <div class="section head">
    <h1>Neighborhood Contrastive Learning for Novel Class Discovery</h1>

    <div class="authors">
      <a href="https://zhunzhong.site/">Zhun Zhong</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://daibo.info/">Enrico Fini</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://liuziwei7.github.io/">Subhankar Roy</a><sup>3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Zhiming Luo</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://luoping.me/">Elisa Ricci</a><sup>1,3</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://luoping.me/">Nicu Sebe</a><sup>1</sup>

    </div>

    <div class="affiliations">
      1. <a href="https://zhunzhong.site/">University of Trento</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      2. <a href="https://zhunzhong.site/">Xiamen University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      3. <a href="https://zhunzhong.site/">Fondazione Bruno Kessler</a>
    </div>
</div>

    <center><img src="./img/ncl_abs.png" border="0" width="95%">
        Illustration of novel class discovery (NCD) and the proposed neighborhood contrastive learning (NCL).
    </center>

<div class="section abstract">
    <h2>Abstract</h2>
    <br>
    <p>
      In this paper, we address Novel Class Discovery (NCD), the task of unveiling new classes in a set of unlabeled samples given a labeled dataset with known classes. We exploit the peculiarities of NCD to build a new framework, named Neighborhood Contrastive Learning (NCL), to learn discriminative representations that are important to clustering performance. Our contribution is twofold. First, we ﬁnd that a feature extractor trained on the labeled set generates representations in which a generic query sample and its neighbors are likely to share the same class. We exploit this observation to retrieve and aggregate pseudo-positive pairs with contrastive learning, thus encouraging the model to learn more discriminative representations. Second, we notice that most of the instances are easily discriminated by the network, contributing less to the contrastive loss. To overcome this issue, we propose to generate hard negatives by mixing labeled and unlabeled samples in the feature space. We experimentally demonstrate that these two ingredients signiﬁcantly contribute to clustering performance and lead our model to outperform state-of-the-art methods by a large margin (e.g., clustering accuracy +13% on CIFAR-100 and +8% on ImageNet).</p>

</div>


<div class="section method">
	<h2>Method Overview</h2>
  <br>
  <center><img src="./img/ncl_framework.png" border="0" width="80%"><br>
    (a) Given a single image, Step 1 initializes the depth with ellipsoid, and optimizes the albedo network A. (b) Step 2 uses the depth and albedo to render `pseudo samples' with various random viewpoint and lighting conditions, and conducts GAN-inversion to them to obtain the `projected samples'. (c) Step 3 refines the depth map by optimizing (V, L, D, A) networks to reconstruct the projected samples. The refined depth and models are used as the new initialization to repeat the above steps.
  </center>
</div>

<div class="section results">
	<h2>More Results</h2>
  <br>
  <center><img src="./img/ncl_results.png" border="0" width="80%"><br>
  </center>
</div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/2011.00844" target="_blank" class="imageLink"><img src="./paper.png"></a><br>
		  <a href="https://arxiv.org/abs/2011.00844" target="_blank">Paper</a>
		</div>
	      </li>

	    </ul>
	    </center>
        </div>

<br>

<div class="section code">
	<h2>Code</h2>
	<center>
	  <ul>

          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/XingangPan/GAN2Shape" target="_blank" class="imageLink"><img src="./code.png"></a><br>
		  <a href="https://github.com/XingangPan/GAN2Shape" target="_blank">Code (to be released)</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section citation">
    <h2>Citation</h2>
    <div class="section bibtex">
      <pre>@article{zhongncl2021,
      title={Neighborhood Contrastive Learning},
      author={Zhong, Zhun and Fini, Enrico and Roy, Subhankar and Luo, Zhiming and Ricci, Elisa and Sebe, Nicu},
      booktitle={Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
      year={2021}
}</pre>
      </div>
      </div>

<div class="section relatedwork">
    <h2>Related Work</h2>
    <br>
    <div class="citation">
      <a href="https://arxiv.org/abs/1912.04958">
        Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila.
        Analyzing and Improving the Image Quality of StyleGAN.
        CVPR 2020.
      </a>
      <br>
    </div>
    <div class="citation">
      <a href="https://elliottwu.com/projects/unsup3d/">
        Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi.
        Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild.
        CVPR 2020.
      </a>
      <br>
    </div>
</div>

</body></html>
