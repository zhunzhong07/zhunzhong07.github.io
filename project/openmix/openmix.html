
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs</title>

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric clues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code and models will be released at https://github.com/XingangPan/GAN2Shape".
>
<meta name="keywords" content="GAN; 3D reconstruction;">
<link rel="author" href="https://xingangpan.github.io/">

<!-- Fonts and stuff -->
<link href="./css" rel="stylesheet" type="text/css">
<link rel="stylesheet" type="text/css" href="./project.css" media="screen">
<link rel="stylesheet" type="text/css" media="screen" href="./iconize.css">
<script async="" src="./prettify.js"></script>

</head>

<body>
  <div id="content">
    <div id="content-inner">
      
    <div class="section head">
    <h1>Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs</h1>

    <div class="authors">
      <a href="https://xingangpan.github.io/">Xingang Pan</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://daibo.info/">Bo Dai</a><sup>1</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="https://liuziwei7.github.io/">Ziwei Liu</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://personal.ie.cuhk.edu.hk/~ccloy/">Chen Change Loy</a><sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      <a href="http://luoping.me/">Ping Luo</a><sup>3</sup>

    </div>

    <div class="affiliations">
      1. <a href="http://www.cuhk.edu.hk/english/index.html">The Chinese University of Hong Kong</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      2. <a href="https://www.ntu.edu.sg/Pages/home.aspx">Nanyang Technological University</a>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
      3. <a href="https://www.cs.hku.hk/">The University of Hong Kong</a>
    </div>
</div>
      
    <center><img src="./GAN2Shape.png" border="0" width="90%"></center>

<div class="section abstract">
    <h2>Abstract</h2>
    <br>
    <p>
      Natural images are projections of 3D objects on a 2D image plane. While state-of-the-art 2D generative models like GANs show unprecedented quality in modeling the natural image manifold, it is unclear whether they implicitly capture the underlying 3D object structures. And if so, how could we exploit such knowledge to recover the 3D shapes of objects in the images? To answer these questions, in this work, we present the first attempt to directly mine 3D geometric clues from an off-the-shelf 2D GAN that is trained on RGB images only. Through our investigation, we found that such a pre-trained GAN indeed contains rich 3D knowledge and thus can be used to recover 3D shape from a single 2D image in an unsupervised manner. The core of our framework is an iterative strategy that explores and exploits diverse viewpoint and lighting variations in the GAN image manifold. The framework does not require 2D keypoint or 3D annotations, or strong assumptions on object shapes (e.g. shapes are symmetric), yet it successfully recovers 3D shapes with high precision for human faces, cats, cars, and buildings. The recovered 3D shapes immediately allow high-quality image editing like relighting and object rotation. We quantitatively demonstrate the effectiveness of our approach compared to previous methods in both 3D shape reconstruction and face rotation. Our code and models will be released at https://github.com/XingangPan/GAN2Shape.</p>

</div>
      
<div class="section demo">
	<h2>Demo</h2>
  <br>
  <center><img src="./GAN2Shape_demo.gif" border="0" width="85%"><br>
    Recovered 3D shape and rotation&relighting effects using GAN2Shape.
  </center>
</div>

<div class="section method">
	<h2>Method Overview</h2>
  <br>
  <center><img src="./GAN2Shape_method.png" border="0" width="70%"><br>
    (a) Given a single image, Step 1 initializes the depth with ellipsoid, and optimizes the albedo network A. (b) Step 2 uses the depth and albedo to render `pseudo samples' with various random viewpoint and lighting conditions, and conducts GAN-inversion to them to obtain the `projected samples'. (c) Step 3 refines the depth map by optimizing (V, L, D, A) networks to reconstruct the projected samples. The refined depth and models are used as the new initialization to repeat the above steps.
  </center>
</div>

<div class="section results">
	<h2>More Results</h2>
  <br>
  <center><img src="./GAN2Shape_appendix.png" border="0" width="80%"><br>
  </center>
</div>

<br>

<div class="section materials">
	<h2>Materials</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://arxiv.org/abs/2011.00844" target="_blank" class="imageLink"><img src="./paper.png"></a><br>
		  <a href="https://arxiv.org/abs/2011.00844" target="_blank">Paper</a>
		</div>
	      </li>
	  
	    </ul>
	    </center>
        </div>
        
<br>

<div class="section code">
	<h2>Code</h2>
	<center>
	  <ul>
           
          <li class="grid">
	      <div class="griditem">
		<a href="https://github.com/XingangPan/GAN2Shape" target="_blank" class="imageLink"><img src="./code.png"></a><br>
		  <a href="https://github.com/XingangPan/GAN2Shape" target="_blank">Code (to be released)</a>
		</div>
	      </li>

	    </ul>
	    </center>
	    </div>

<br>

<div class="section citation">
    <h2>Citation</h2>
    <div class="section bibtex">
      <pre>@article{pan2020gan2shape,
    title   = {Do 2D GANs Know 3D Shape? Unsupervised 3D Shape Reconstruction from 2D Image GANs},
    author  = {Pan, Xingang and Dai, Bo and Liu, Ziwei and Loy, Chen Change and Luo, Ping},
    journal = {arXiv preprint arXiv:2011.00844},
    year    = {2020}
}</pre>
      </div>
      </div>

<div class="section relatedwork">
    <h2>Related Work</h2>
    <br>
    <div class="citation">
      <a href="https://arxiv.org/abs/1912.04958">
        Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, Timo Aila.
        Analyzing and Improving the Image Quality of StyleGAN.
        CVPR 2020.
      </a>
      <br>
    </div>
    <div class="citation">
      <a href="https://elliottwu.com/projects/unsup3d/">
        Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi.
        Unsupervised Learning of Probably Symmetric Deformable 3D Objects from Images in the Wild.
        CVPR 2020.
      </a>
      <br>
    </div>
</div>

</body></html>
